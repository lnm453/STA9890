---
title: "STA9890 Project"
author: "Mai Le"
date: "5/7/2020"
output: word_document
---

#```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
#```

```{r, message=FALSE, warning=FALSE}
setwd("/Users/Mai/Google Drive/Grad/2020 Spring/STA 9890/STA9890 Project/")
library(tidyverse)
library(dplyr) 
library(glmnet)
library("haven")
library(randomForest)
library(gridExtra)
```
## Loading data

```{r, message=FALSE, warning=FALSE}
data <- read_csv("us_county_sociohealth_data.csv")
```



## Data Pre-processing
```{r, message=FALSE, warning=FALSE}
# converting categorical variables to factor
data$presence_of_water_violation <- as.factor(data$presence_of_water_violation)
data$state <- as.factor(data$state)
data$county <- as.factor(data$county)
```

### Impute missing values
```{r, message=FALSE, warning=FALSE}
f.index <- grep("presence_of_water_violation", colnames(data))

# Impute missing data-points with their mean
for(i in 6:(f.index-1)) {
  for (j in 1:nrow(data)) {
    data[j,i] <- ifelse(is.na(data[j,i]), mean(data.matrix(data[,i]), na.rm=TRUE), data[j,i])
  }
}

for(i in (f.index+1):ncol(data)) {
  for (j in 1:nrow(data)) {
    data[j,i] <- ifelse(is.na(data[j,i]), mean(data.matrix(data[,i]), na.rm=TRUE), data[j,i])
  }
}

# imput missing data points for logical variable 'presence_of_water_violation'
find.mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

mode <- find.mode(data[, f.index])[1]

for (i in 1:nrow(data)) {
  data[i, f.index] <- ifelse(is.na(data[i, f.index]), "FALSE", data[i, f.index])
}
```

```{r, message=FALSE, warning=FALSE}
# Double check number of missing values
sum(is.na(data))
# Remove columns 'lat', 'lon' and 'fips'
data.orig <- data
data <- select(data, -c(lat, lon, fips, county))
```

### Standardize numeric predictors
```{r, message=FALSE, warning=FALSE}
# Get predictor values 
data_predictors <- select(data, -years_of_potential_life_lost_rate)
# Standardize all numeric predictors based on equation 6.6 in ISLR
predictor_std <- as.data.frame(lapply(data_predictors, function(x) if(is.numeric(x)){
  x/sd(x)
} else x))

apply(predictor_std, 2, 'sd')
```



## Model Building on 100 samples
```{r, message=FALSE, warning=FALSE}
n        =    dim(data)[1]
p        =    dim(data_predictors)[2]
res.index <- grep("years_of_potential_life_lost_rate", colnames(data))
y        =   data.matrix(log(data[,res.index]))
X        =   data.matrix(predictor_std)
```

```{r, message=FALSE, warning=FALSE}
n.train        =     floor(0.8*n)
n.test         =     n-n.train

M              =     100
#R-squared values for each model
Rsq.test.rf    =    rep(0,M)  # rf= randomForest
Rsq.train.rf   =    rep(0,M)
Rsq.test.rid   =    rep(0,M)  # rid = ridgeRegression
Rsq.train.rid  =    rep(0,M)
Rsq.test.en    =    rep(0,M)  #en = elastic net
Rsq.train.en   =    rep(0,M)
Rsq.test.las   =    rep(0,M)  #las = lasso
Rsq.train.las  =    rep(0,M)


for (m in c(1:M)) {
  shuffled_indexes =     sample(n)
  train            =     shuffled_indexes[1:n.train]
  test             =     shuffled_indexes[(1+n.train):n]
  X.train          =     X[train, ]
  y.train          =     y[train]
  X.test           =     X[test, ]
  y.test           =     y[test]
  
  # fit RF and calculate and record the train and test R squares 
  rf               =     randomForest(X.train, y.train, mtry = sqrt(p), importance = TRUE)
  y.test.hat       =     predict(rf, X.test)
  y.train.hat      =     predict(rf, X.train)
  Rsq.test.rf[m]   =     1-mean((y.test - y.test.hat)^2)/mean((y - mean(y))^2)
  Rsq.train.rf[m]  =     1-mean((y.train - y.train.hat)^2)/mean((y - mean(y))^2)
  
  # fit ridge regression and calculate and record the train and test R squares 
  a=0 # elastic-net
  cv.fit           =     cv.glmnet(X.train, y.train, alpha = a, nfolds = 10)
  fit              =     glmnet(X.train, y.train, alpha = a, lambda = cv.fit$lambda.min)
  y.train.hat      =     predict(fit, newx = X.train, type = "response") 
  y.test.hat       =     predict(fit, newx = X.test, type = "response") 
  Rsq.test.rid[m]   =     1-mean((y.test - y.test.hat)^2)/mean((y - mean(y))^2)
  Rsq.train.rid[m]  =     1-mean((y.train - y.train.hat)^2)/mean((y - mean(y))^2)
  
  # fit elastic-net and calculate and record the train and test R squares 
  a=0.5 # elastic-net
  cv.fit           =     cv.glmnet(X.train, y.train, alpha = a, nfolds = 10)
  fit              =     glmnet(X.train, y.train, alpha = a, lambda = cv.fit$lambda.min)
  y.train.hat      =     predict(fit, newx = X.train, type = "response") 
  y.test.hat       =     predict(fit, newx = X.test, type = "response") 
  Rsq.test.en[m]   =     1-mean((y.test - y.test.hat)^2)/mean((y - mean(y))^2)
  Rsq.train.en[m]  =     1-mean((y.train - y.train.hat)^2)/mean((y - mean(y))^2)
  
  # fit lasso and calculate and record the train and test R squares 
  a=1 # elastic-net
  cv.fit           =     cv.glmnet(X.train, y.train, alpha = a, nfolds = 10)
  fit              =     glmnet(X.train, y.train, alpha = a, lambda = cv.fit$lambda.min)
  y.train.hat       =    predict(fit, newx = X.train, type = "response") 
  y.test.hat        =    predict(fit, newx = X.test, type = "response") 
  Rsq.test.las[m]   =    1-mean((y.test - y.test.hat)^2)/mean((y - mean(y))^2)
  Rsq.train.las[m]  =    1-mean((y.train - y.train.hat)^2)/mean((y - mean(y))^2)
  
  cat(sprintf("m=%3.f| Rsq.test.rf=%.2f,  Rsq.test.rid=%.2f,  Rsq.test.en=%.2f,  Rsq.test.las=%.2f|\n     | Rsq.train.rf=%.2f,  Rsq.train.rid=%.2f,  Rsq.train.en=%.2f,  Rsq.train.las=%.2f| \n", m,  Rsq.test.rf[m], Rsq.test.rid[m], Rsq.test.en[m], Rsq.test.las[m], Rsq.train.rf[m], Rsq.train.rid[m], Rsq.train.en[m], Rsq.train.las[m]))
  
}
```

```{r, message=FALSE, warning=FALSE}
#side-by-side boxplots of Rsq test and train
par(mfrow=c(1,2))
boxplot(Rsq.train.rf, Rsq.train.rid, Rsq.train.en, Rsq.train.las,
        main = "R-squared train values",
        at = c(1,4,7,10),
        names = c("RF", "Ridge", "E-net", "Lasso"),
        ylim=c(0.85,1.00))
boxplot(Rsq.test.rf, Rsq.test.rid, Rsq.test.en, Rsq.test.las,
        main = "R-squared test values",
        at = c(1,4,7,10),
        names = c("RF", "Ridge", "E-net", "Lasso"),
        ylim=c(0.85,1.00))
```



## 10-fold CV on one of 100 samples
```{r, message=FALSE, warning=FALSE}
#Train and test residuals for each model
err.test.rid   =    rep(0,n.test)  # rid = ridgeRegression
err.train.rid  =    rep(0,n.train)
err.test.en    =    rep(0,n.test)  #en = elastic net
err.train.en   =    rep(0,n.train)
err.test.las   =    rep(0,n.test)  #las = lasso
err.train.las  =    rep(0,n.train)

a= 0 #Ridge
cv.fit       =     cv.glmnet(X.train, y.train, alpha = a, nfolds=10)
plot(cv.fit, main = 'CV error for ridge regression')
fit          =     glmnet(X.train, y.train, alpha = a, lambda = cv.fit$lambda.min)
y.train.hat  =     predict(fit, newx = X.train, type = "response") 
y.test.hat   =     predict(fit, newx = X.test, type = "response") 
err.train.rid    =     as.vector(y.train.hat - y.train)
err.test.rid     =     as.vector(y.test.hat - y.test)


a= 0.5 #Elastic-net
cv.fit       =     cv.glmnet(X.train, y.train, alpha = a, nfolds=10)
plot(cv.fit, main = 'CV error for elastic-net')
fit          =     glmnet(X.train, y.train, alpha = a, lambda = cv.fit$lambda.min)
y.train.hat  =     predict(fit, newx = X.train, type = "response") 
y.test.hat   =     predict(fit, newx = X.test, type = "response") 
err.train.en    =     as.vector(y.train.hat - y.train)
err.test.en     =     as.vector(y.test.hat - y.test)


a= 1 #Lasso
cv.fit       =     cv.glmnet(X.train, y.train, alpha = a, nfolds=10)
plot(cv.fit, main = 'CV error for the lasso')
fit          =     glmnet(X.train, y.train, alpha = a, lambda = cv.fit$lambda.min)
y.train.hat  =     predict(fit, newx = X.train, type = "response") 
y.test.hat   =     predict(fit, newx = X.test, type = "response") 
err.train.las    =     as.vector(y.train.hat - y.train)
err.test.las     =     as.vector(y.test.hat - y.test)


par(mfrow=c(1,2))
boxplot(err.train.rid, err.train.en, err.train.las,
        main = "Train residuals",
        names = c("Ridge", "E-net", "Lasso"),
        ylim=c(-0.6,0.6))
boxplot(err.test.rid, err.test.en, err.test.las,
        main = "Test residuals",
        names = c("Ridge", "E-net", "Lasso"),
        ylim=c(-0.6,0.6))
```


## Bootstraping
```{r, message=FALSE, warning=FALSE}
bootstrapSamples =     100
beta.rf.bs       =     matrix(0, nrow = p, ncol = bootstrapSamples)
beta.rid.bs       =     matrix(0, nrow = p, ncol = bootstrapSamples)
beta.en.bs       =     matrix(0, nrow = p, ncol = bootstrapSamples)
beta.las.bs       =     matrix(0, nrow = p, ncol = bootstrapSamples)

for (m in 1:bootstrapSamples){
  bs_indexes       =     sample(n, replace=T)
  X.bs             =     X[bs_indexes, ]
  y.bs             =     data.matrix(y[bs_indexes])

  # fit bs rf
  rf               =     randomForest(X.bs, y.bs, mtry = sqrt(p), importance = TRUE)
  beta.rf.bs[,m]   =     as.vector(rf$importance[,1])

  # fit bs rid
  a                =     0 # rid
  cv.fit           =     cv.glmnet(X.bs, y.bs, alpha = a, nfolds = 10)
  fit              =     glmnet(X.bs, y.bs, alpha = a, lambda = cv.fit$lambda.min)  
  beta.rid.bs[,m]   =     as.vector(fit$beta)
  
  # fit bs en
  a                =     0.5 # elastic-net
  cv.fit           =     cv.glmnet(X.bs, y.bs, alpha = a, nfolds = 10)
  fit              =     glmnet(X.bs, y.bs, alpha = a, lambda = cv.fit$lambda.min)  
  beta.en.bs[,m]   =     as.vector(fit$beta)
  
  # fit bs lasso
  a                =     1 # lasso
  cv.fit           =     cv.glmnet(X.bs, y.bs, alpha = a, nfolds = 10)
  fit              =     glmnet(X.bs, y.bs, alpha = a, lambda = cv.fit$lambda.min)  
  beta.las.bs[,m]   =    as.vector(fit$beta)
  cat(sprintf("Bootstrap Sample %3.f \n", m))
}

# calculate bootstrapped standard errors / alternatively you could use qunatiles to find upper and lower bounds
rf.bs.sd    = apply(beta.rf.bs, 1, "sd")
rid.bs.sd    = apply(beta.rid.bs, 1, "sd")
en.bs.sd    = apply(beta.en.bs, 1, "sd")
las.bs.sd    = apply(beta.las.bs, 1, "sd")


# fit rf to the whole data
rf               =     randomForest(X, y, mtry = sqrt(p), importance = TRUE)
betaS.rf               =     data.frame(c(1:p), as.vector(rf$importance[,1]), 2*rf.bs.sd)
colnames(betaS.rf)     =     c( "feature", "value", "err")

# fit rid to the whole data
a=0 # rid
cv.fit           =     cv.glmnet(X, y, alpha = a, nfolds = 10)
fit              =     glmnet(X, y, alpha = a, lambda = cv.fit$lambda.min)
betaS.rid             =     data.frame(c(1:p), as.vector(fit$beta), 2*rid.bs.sd)
colnames(betaS.rid)    =     c( "feature", "value", "err")

# fit en to the whole data
a=0.5 # elastic-net
cv.fit           =     cv.glmnet(X, y, alpha = a, nfolds = 10)
fit              =     glmnet(X, y, alpha = a, lambda = cv.fit$lambda.min)
betaS.en               =     data.frame(c(1:p), as.vector(fit$beta), 2*en.bs.sd)
colnames(betaS.en)     =     c( "feature", "value", "err")

# fit lasso to the whole data
a=1 # las
cv.fit           =     cv.glmnet(X, y, alpha = a, nfolds = 10)
fit              =     glmnet(X, y, alpha = a, lambda = cv.fit$lambda.min)
betaS.las               =     data.frame(c(1:p), as.vector(fit$beta), 2*las.bs.sd)
colnames(betaS.las)     =     c( "feature", "value", "err")


# we need to change the order of factor levels by specifying the order explicitly.
betaS.rf$feature     =  factor(betaS.rf$feature, levels = betaS.rf$feature[order(betaS.rf$value, decreasing = TRUE)])
betaS.rid$feature     =  factor(betaS.rid$feature, levels = betaS.rf$feature[order(betaS.rf$value, decreasing = TRUE)])
betaS.en$feature     =  factor(betaS.en$feature, levels = betaS.rf$feature[order(betaS.rf$value, decreasing = TRUE)])
betaS.las$feature     =  factor(betaS.las$feature, levels = betaS.rf$feature[order(betaS.rf$value, decreasing = TRUE)])



rfPlot =  ggplot(betaS.rf, aes(x=feature, y=value)) +
  geom_bar(stat = "identity", fill="white", colour="black")    +
  geom_errorbar(aes(ymin=value-err, ymax=value+err), width=.2) + 
  theme(axis.text.x = element_text(angle = 90)) +
  xlab("Random Forest features") 

ridPlot =  ggplot(betaS.rid, aes(x=feature, y=value)) +
  geom_bar(stat = "identity", fill="white", colour="black")    +
  geom_errorbar(aes(ymin=value-err, ymax=value+err), width=.2) + 
  theme(axis.text.x = element_text(angle = 90)) +
  xlab("Ridge Regression features") 

enPlot =  ggplot(betaS.en, aes(x=feature, y=value)) +
  geom_bar(stat = "identity", fill="white", colour="black")    +
  geom_errorbar(aes(ymin=value-err, ymax=value+err), width=.2) + 
  theme(axis.text.x = element_text(angle = 90)) +
  xlab("Elastic-net features") 

lasPlot =  ggplot(betaS.las, aes(x=feature, y=value)) +
  geom_bar(stat = "identity", fill="white", colour="black")    +
  geom_errorbar(aes(ymin=value-err, ymax=value+err), width=.2) + 
  theme(axis.text.x = element_text(angle = 90)) +
  xlab("Lasso features") 


grid.arrange(rfPlot, ridPlot, enPlot, lasPlot, nrow = 4)
```

```{r}
# List of predictors used in the models
cols <- as.data.frame( data_predictors[1,], drop=false)
cols <- data.frame(t(cols[-1]))
write.csv(cols,'varlist.csv')
```

```{r}
# Check the computational time it takes to train each model
system.time(fit <-randomForest(X[1:n.train,], y[1:n.train], mtry = sqrt(n.train), importance = TRUE))     #rf
system.time(fit <- cv.glmnet(X[1:n.train,], y[1:n.train], alpha = 0, nfolds = 10))   #ridge
system.time(fit <- cv.glmnet(X[1:n.train,], y[1:n.train], alpha = 0.5, nfolds = 10))  #en
system.time(fit <- cv.glmnet(X[1:n.train,], y[1:n.train], alpha = 1, nfolds = 10))   #las
```